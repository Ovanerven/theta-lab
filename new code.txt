kinetics_rnn.py: 

from __future__ import annotations
from typing import List, Optional, Tuple

import torch
import torch.nn as nn

from src.sim.mechanisms import (
    full13_rhs, reduced5_rhs, reduced7_rhs, reduced8_rhs, reduced9_rhs,
)


def gamma_sigmoid(x: torch.Tensor, lo: float, hi: float) -> torch.Tensor:
    return lo + (hi - lo) * torch.sigmoid(x)


# ---------------------------------------------------------------------------
# Concrete RK4 integrators per mechanism — required for torch.jit.script
# (TorchScript cannot call arbitrary Python callables, so each mechanism
#  needs its own inlined loop.)
# ---------------------------------------------------------------------------

def rk4_full13(y: torch.Tensor, dt: torch.Tensor, theta: torch.Tensor, n_sub: int) -> torch.Tensor:
    h = dt / float(n_sub)
    for _ in range(n_sub):
        k1 = full13_rhs(y, theta)
        k2 = full13_rhs(y + 0.5 * h * k1, theta)
        k3 = full13_rhs(y + 0.5 * h * k2, theta)
        k4 = full13_rhs(y + h * k3, theta)
        y = torch.clamp_min(y + (h / 6.0) * (k1 + 2*k2 + 2*k3 + k4), 0.0)
    return y


def rk4_reduced5(y: torch.Tensor, dt: torch.Tensor, theta: torch.Tensor, n_sub: int) -> torch.Tensor:
    h = dt / float(n_sub)
    for _ in range(n_sub):
        k1 = reduced5_rhs(y, theta)
        k2 = reduced5_rhs(y + 0.5 * h * k1, theta)
        k3 = reduced5_rhs(y + 0.5 * h * k2, theta)
        k4 = reduced5_rhs(y + h * k3, theta)
        y = torch.clamp_min(y + (h / 6.0) * (k1 + 2*k2 + 2*k3 + k4), 0.0)
    return y


def rk4_reduced7(y: torch.Tensor, dt: torch.Tensor, theta: torch.Tensor, n_sub: int) -> torch.Tensor:
    h = dt / float(n_sub)
    for _ in range(n_sub):
        k1 = reduced7_rhs(y, theta)
        k2 = reduced7_rhs(y + 0.5 * h * k1, theta)
        k3 = reduced7_rhs(y + 0.5 * h * k2, theta)
        k4 = reduced7_rhs(y + h * k3, theta)
        y = torch.clamp_min(y + (h / 6.0) * (k1 + 2*k2 + 2*k3 + k4), 0.0)
    return y


def rk4_reduced8(y: torch.Tensor, dt: torch.Tensor, theta: torch.Tensor, n_sub: int) -> torch.Tensor:
    h = dt / float(n_sub)
    for _ in range(n_sub):
        k1 = reduced8_rhs(y, theta)
        k2 = reduced8_rhs(y + 0.5 * h * k1, theta)
        k3 = reduced8_rhs(y + 0.5 * h * k2, theta)
        k4 = reduced8_rhs(y + h * k3, theta)
        y = torch.clamp_min(y + (h / 6.0) * (k1 + 2*k2 + 2*k3 + k4), 0.0)
    return y


def rk4_reduced9(y: torch.Tensor, dt: torch.Tensor, theta: torch.Tensor, n_sub: int) -> torch.Tensor:
    h = dt / float(n_sub)
    for _ in range(n_sub):
        k1 = reduced9_rhs(y, theta)
        k2 = reduced9_rhs(y + 0.5 * h * k1, theta)
        k3 = reduced9_rhs(y + 0.5 * h * k2, theta)
        k4 = reduced9_rhs(y + h * k3, theta)
        y = torch.clamp_min(y + (h / 6.0) * (k1 + 2*k2 + 2*k3 + k4), 0.0)
    return y


class KineticsRNN(nn.Module):
    def __init__(
        self,
        mech,                            # Mechanism from MECH dict
        obs_indices: List[int],          # which full-state species are observed
        in_u: int,
        u_to_y_jump: torch.Tensor,       # (U, P): maps control -> observed species
        lift_dim: int = 32,
        hidden: int = 128,
        num_layers: int = 1,
        dropout: float = 0.0,
        n_sub: int = 1,
    ):
        super().__init__()
        self.mech_name: str = mech.name   # stored as str for TorchScript dispatch
        self.obs_indices = obs_indices     # metadata for checkpointing / plotting
        self.P: int = len(obs_indices)
        self.n_state: int = mech.n_state
        self.n_param: int = mech.n_param
        self.n_sub = n_sub

        self.lift = nn.Sequential(
            nn.Linear(in_u + self.P, lift_dim),
            nn.SiLU(),
            nn.Dropout(dropout) if dropout > 0 else nn.Identity(),
        )
        self.gru = nn.GRU(
            input_size=lift_dim,
            hidden_size=hidden,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0.0,
        )
        self.head = nn.Linear(hidden, mech.n_param)

        self.register_buffer("u_to_y_jump", u_to_y_jump.to(torch.float32))
        self.register_buffer("obs_idx_buf", torch.tensor(obs_indices, dtype=torch.long))

        for m in self.lift:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
        for name, p in self.gru.named_parameters():
            if "weight" in name:
                nn.init.xavier_uniform_(p)
            elif "bias" in name:
                nn.init.zeros_(p)
        nn.init.xavier_uniform_(self.head.weight)
        nn.init.zeros_(self.head.bias)

    def forward(
        self,
        y0: torch.Tensor,                          # (B, P)
        u_seq: torch.Tensor,                       # (B, K, U)
        dt_seq: torch.Tensor,                      # (B, K)
        y_seq: Optional[torch.Tensor] = None,      # (B, K, P) for teacher forcing
        teacher_forcing: bool = True,
        tf_every: int = 50,
    ) -> Tuple[torch.Tensor, torch.Tensor]:

        B, K, U = u_seq.shape
        dev = u_seq.device
        dtype = u_seq.dtype

        obs_idx = self.obs_idx_buf.to(device=dev)  # actual positions in n_state vector
        y_hat     = torch.empty(B, K, self.P,       device=dev, dtype=dtype)
        theta_out = torch.empty(B, K, self.n_param, device=dev, dtype=dtype)

        y_full = torch.zeros(B, self.n_state, device=dev, dtype=dtype)
        y_full[:, obs_idx] = y0

        h = torch.zeros(self.gru.num_layers, B, self.gru.hidden_size, device=dev, dtype=dtype)

        for k in range(K):
            u_k  = u_seq[:, k, :]      # (B, U)
            dt_k = dt_seq[:, k:k+1]   # (B, 1)
            y_obs = y_full[:, obs_idx] # (B, P)

            if teacher_forcing and (y_seq is not None) and k > 0 and (k % tf_every == 0):
                y_in = y_seq[:, k-1, :].detach()
            else:
                y_in = y_obs.detach()

            feat = torch.cat([u_k, y_in], dim=-1)
            x, h = self.gru(self.lift(feat).unsqueeze(1), h)
            theta_k = gamma_sigmoid(self.head(x.squeeze(1)), 1e-3, 2.0)

            # apply bolus to observed species
            y_full_jump = y_full.clone()
            y_full_jump[:, obs_idx] = y_obs + (u_k @ self.u_to_y_jump.to(dtype=dtype))

            # integrate — dispatch by mech_name (required for TorchScript)
            if self.mech_name == "full13":
                y_full = rk4_full13(y_full_jump, dt_k, theta_k, self.n_sub)
            elif self.mech_name == "reduced7":
                y_full = rk4_reduced7(y_full_jump, dt_k, theta_k, self.n_sub)
            elif self.mech_name == "reduced8":
                y_full = rk4_reduced8(y_full_jump, dt_k, theta_k, self.n_sub)
            elif self.mech_name == "reduced9":
                y_full = rk4_reduced9(y_full_jump, dt_k, theta_k, self.n_sub)
            else:
                y_full = rk4_reduced5(y_full_jump, dt_k, theta_k, self.n_sub)

            y_hat[:, k, :]     = y_full[:, obs_idx]
            theta_out[:, k, :] = theta_k

        return (y_hat, theta_out) if self.training else (y_hat, theta_out.detach())

train.py:

from __future__ import annotations
import argparse
import time
from pathlib import Path
from typing import Optional

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import yaml

from src.data.ode_dataset import ODEDataset, collate
from src.sim.mechanisms import MECH
from src.models.kinetics_rnn import KineticsRNN


# def loss_fn(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
#     return (torch.log1p(pred) - torch.log1p(target)).pow(2).mean()

def loss_fn(pred: torch.Tensor, target: torch.Tensor, alpha: float = 0.2) -> torch.Tensor:
    lin = (pred - target).pow(2)
    log = (torch.log1p(pred) - torch.log1p(target)).pow(2)
    return (alpha * lin + (1 - alpha) * log).mean()


def per_species_diag(
    pred: torch.Tensor,
    target: torch.Tensor,
    species_names: list[str],
    ep: int,
) -> None:
    """Print per-species losses in three spaces for a single batch/epoch.

    pred, target: (B, T, P) — already on CPU
    """
    eps = 1e-8
    P = pred.shape[-1]
    header = f"  {'species':>10}  {'MSE-lin':>12}  {'MSE-log1p':>12}  {'MSE-rel':>12}"
    print(f"\n--- per-species diagnostic  ep {ep} ---")
    print(header)
    for p in range(P):
        pr  = pred[..., p]
        tg  = target[..., p]
        mse_lin   = float((pr - tg).pow(2).mean())
        mse_log1p = float((torch.log1p(pr) - torch.log1p(tg)).pow(2).mean())
        mse_rel   = float(((pr - tg) / (tg + eps)).pow(2).mean())
        name = species_names[p] if p < len(species_names) else str(p)
        print(f"  {name:>10}  {mse_lin:>12.6f}  {mse_log1p:>12.6f}  {mse_rel:>12.6f}")
    print()


def pick_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda")
    if torch.backends.mps.is_available(): # also runs on mac
        return torch.device("mps")
    return torch.device("cpu")


def train_from_config(config_path: str | Path) -> None:
    config_path = Path(config_path)
    cfg = yaml.safe_load(config_path.read_text())
    t_start = time.time()

    ds_cfg   = cfg["dataset"]
    mech_cfg = cfg["mechanism"]
    tr_cfg   = cfg.get("train", {})
    out_cfg  = cfg.get("output", {})

    seed = int(tr_cfg.get("seed", 42))
    np.random.seed(seed)
    torch.manual_seed(seed)

    device = pick_device()
    print(f"Using device: {device}")

    mech_name = mech_cfg["name"]
    mech = MECH[mech_name]
    print(f"Mechanism: {mech_name} | n_state={mech.n_state} n_param={mech.n_param}")

    dataset_path = Path(ds_cfg["path"])
    ds = ODEDataset(dataset_path, obs_indices=ds_cfg.get("obs_indices", None))
    print(f"Dataset: {dataset_path.name} | N={len(ds)} P={ds.y0.shape[1]} K={ds.u_seq.shape[1]}")
    print(f"obs_indices (effective): {ds.obs_indices.tolist()}")

    N = len(ds)
    rng = np.random.default_rng(seed)
    perm = rng.permutation(N)
    val_frac = float(tr_cfg.get("val_frac", 0.15))
    n_val = max(1, int(N * val_frac)) if (val_frac > 0 and N > 1) else 0
    val_idx   = perm[:n_val].tolist()
    train_idx = perm[n_val:].tolist()

    batch = int(tr_cfg.get("batch", 300))
    train_loader = DataLoader(
        torch.utils.data.Subset(ds, train_idx), batch_size=batch,
        shuffle=True, num_workers=0, collate_fn=collate, pin_memory=True,
    )
    val_loader = None
    if n_val > 0:
        val_loader = DataLoader(
            torch.utils.data.Subset(ds, val_idx), batch_size=batch,
            shuffle=False, num_workers=0, collate_fn=collate, pin_memory=True,
        )

    y0_ex, u_ex, _, _ = ds[0]
    P = int(y0_ex.shape[0])
    U = int(u_ex.shape[1])
    obs_full  = ds.obs_indices.tolist()   # file column numbers, e.g. [0,3,6,9,12]
    ctrl_full = ds.control_indices.tolist()

    # obs_indices for the model = positions within mech.n_state vector.
    # For full13 (n_state=13), file columns ARE state indices.
    # For reduced models (n_state=P), the state vector only has P slots: use 0..P-1.
    if mech.n_state == 13:
        model_obs = obs_full          # full13: file columns map directly to state slots
    else:
        model_obs = list(range(P))    # reduced: all P observed species fill slots 0..P-1

    obs_pos = {full_idx: p for p, full_idx in enumerate(obs_full)}
    jump = torch.zeros(U, P, dtype=torch.float32)
    for u_i, full_idx in enumerate(ctrl_full):
        p = obs_pos.get(full_idx)
        if p is not None:
            jump[u_i, p] = 1.0

    model = KineticsRNN(
        mech=mech,
        obs_indices=model_obs,
        in_u=U,
        u_to_y_jump=jump,
        hidden=int(tr_cfg.get("hidden", 128)),
        num_layers=int(tr_cfg.get("num_layers", 1)),
        dropout=float(tr_cfg.get("dropout", 0.0)),
        n_sub=int(tr_cfg.get("n_sub", 1)),
    ).to(device)

    if tr_cfg.get("jit_script", True):
        try:
            model = torch.jit.script(model)
            print("TorchScript: OK")
        except Exception as e:
            print(f"TorchScript failed (running eagerly): {e}")

    opt = torch.optim.AdamW(
        model.parameters(),
        lr=float(tr_cfg.get("lr", 1e-3)),
        weight_decay=float(tr_cfg.get("weight_decay", 0.0085)),
    )

    epochs       = int(tr_cfg.get("epochs", 200))
    grad_clip    = float(tr_cfg.get("grad_clip", 1.0))
    tf_every     = int(tr_cfg.get("tf_every", 50))
    tf_drop_ep   = int(tr_cfg.get("tf_drop_epoch", 250))
    tf_enabled   = bool(tr_cfg.get("teacher_forcing", True))
    ckpt_every   = int(out_cfg.get("checkpoint_every", 0))
    diag_every   = int(out_cfg.get("diag_every", 10))  # 0 = disabled

    exp_name = out_cfg.get("exp_name", config_path.stem)
    exp_dir  = Path("experiments") / exp_name
    exp_dir.mkdir(parents=True, exist_ok=True)
    log_dir  = exp_dir / "logs"
    log_dir.mkdir(exist_ok=True)
    model_path = exp_dir / "model.pt"

    best_val   = float("inf")
    best_state = None
    train_losses: list[float] = []
    val_losses:   list[float] = []

    for ep in range(1, epochs + 1):
        t0 = time.time()
        use_tf = tf_enabled and (ep < tf_drop_ep)

        model.train()
        total, nb = 0.0, 0
        for y0, u_seq, dt_seq, y_seq in train_loader:
            y0     = y0.to(device)
            u_seq  = u_seq.to(device)
            dt_seq = dt_seq.to(device)
            y_seq  = y_seq.to(device)

            opt.zero_grad()
            pred, _ = model(y0, u_seq, dt_seq, y_seq=y_seq, teacher_forcing=use_tf, tf_every=tf_every)
            loss = loss_fn(pred, y_seq)
            loss.backward()
            if grad_clip > 0:
                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            opt.step()
            total += float(loss.item())
            nb += 1

        train_losses.append(total / max(1, nb))

        va_loss = None
        if val_loader is not None:
            model.eval()
            vtotal, vb = 0.0, 0
            diag_preds:   list[torch.Tensor] = []
            diag_targets: list[torch.Tensor] = []
            run_diag = diag_every > 0 and ep % diag_every == 0
            with torch.no_grad():
                for y0, u_seq, dt_seq, y_seq in val_loader:
                    y0     = y0.to(device)
                    u_seq  = u_seq.to(device)
                    dt_seq = dt_seq.to(device)
                    y_seq  = y_seq.to(device)
                    pred, _ = model(y0, u_seq, dt_seq, teacher_forcing=False)
                    vtotal += float(loss_fn(pred, y_seq).item())
                    vb += 1
                    if run_diag:
                        diag_preds.append(pred.cpu())
                        diag_targets.append(y_seq.cpu())
            va_loss = vtotal / max(1, vb)
            if run_diag:
                all_pred   = torch.cat(diag_preds,   dim=0)
                all_target = torch.cat(diag_targets, dim=0)
                per_species_diag(all_pred, all_target, mech.state_names, ep)
            val_losses.append(va_loss)
            if va_loss < best_val:
                best_val = va_loss
                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}

        dt = time.time() - t0
        if va_loss is None:
            print(f"ep {ep:4d} | train {train_losses[-1]:.6f} | {dt:.2f}s")
        else:
            print(f"ep {ep:4d} | train {train_losses[-1]:.6f} | val {va_loss:.6f} | best {best_val:.6f} | {dt:.2f}s")

        if ckpt_every > 0 and ep % ckpt_every == 0:
            ckpt_dir = log_dir / "checkpoints"
            ckpt_dir.mkdir(exist_ok=True)
            state = {k.replace("_orig_mod.", ""): v.detach().cpu() for k, v in model.state_dict().items()}
            torch.save({"state_dict": state, "epoch": ep, "mechanism": mech_name,
                        "obs_indices": model_obs, "cfg": cfg}, ckpt_dir / f"model_ep{ep:04d}.pt")

    if best_state is not None:
        model.load_state_dict(best_state)

    np.savez(log_dir / "loss_curves.npz",
             train_losses=np.array(train_losses),
             val_losses=np.array(val_losses) if val_losses else np.array([]))

    final_state = {k.replace("_orig_mod.", ""): v.detach().cpu() for k, v in model.state_dict().items()}
    torch.save({"state_dict": final_state, "mechanism": mech_name,
                "obs_indices": model_obs, "cfg": cfg, "best_val": best_val}, model_path)

    (exp_dir / "config_used.yaml").write_text(yaml.dump(cfg, sort_keys=False))
    print(f"Saved model → {model_path}")
    print(f"Done in {time.time() - t_start:.1f}s")

    try:
        from src.scripts.plotting import plot_loss_curves, plot_predictions, plot_learned_parameters, plot_evolution
        plots_dir = exp_dir / "plots"
        plots_dir.mkdir(exist_ok=True)
        plot_loss_curves(log_dir / "loss_curves.npz", plots_dir)
        plot_predictions(model_path, dataset_path, plots_dir)
        plot_learned_parameters(str(model_path), str(dataset_path), output_path=str(plots_dir / "theta_sample0.png"))
        ckpt_dir = log_dir / "checkpoints"
        if ckpt_dir.exists() and any(ckpt_dir.glob("model_ep*.pt")):
            plot_evolution(str(log_dir), str(dataset_path), out_dir=str(plots_dir))
        print(f"Plots saved → {plots_dir}")
    except Exception as e:
        print(f"Auto-plotting failed (skipping): {e}")


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()
    train_from_config(args.config)

mechanisms.py:

from __future__ import annotations
from dataclasses import dataclass
from typing import Callable, List

import torch


@dataclass(frozen=True)
class Mechanism:
    name: str
    n_state: int
    n_param: int
    state_names: List[str]
    param_names: List[str]
    rhs: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]  # (B,n_state),(B,n_param)->(B,n_state)


# -------------------------
# RHS definitions
# -------------------------
# Here we can define different mechanisms by writing out their ODE right-hand sides. 
def full13_rhs(y: torch.Tensor, k: torch.Tensor) -> torch.Tensor:
    A, B, C, D, E, F, G, H, I, J, K, L, M = y.unbind(dim=-1)
    (kf1, kf2, kf3, kf4, kf5, kf6, kf7, kf8, kf9, kf10, kf11, kf12,
     kr1, kr3, kr5, kr7, kr9, kr11, kr12) = k.unbind(dim=-1)

    dA = -kf1*A  + kr1*B
    dB =  kf1*A  - kr1*B  - kf2*B
    dC =  kf2*B  - kf3*C  + kr3*D
    dD =  kf3*C  - kr3*D  - kf4*D
    dE =  kf4*D  - kf5*E  + kr5*F
    dF =  kf5*E  - kr5*F  - kf6*F
    dG =  kf6*F  - kf7*G  + kr7*H
    dH =  kf7*G  - kr7*H  - kf8*H
    dI =  kf8*H  - kf9*I  + kr9*J
    dJ =  kf9*I  - kr9*J  - kf10*J
    dK =  kf10*J - kf11*K + kr11*L
    dL =  kf11*K - kr11*L - kf12*L + kr12*M
    dM =  kf12*L - kr12*M

    return torch.stack([dA, dB, dC, dD, dE, dF, dG, dH, dI, dJ, dK, dL, dM], dim=-1)


def reduced5_rhs(y: torch.Tensor, k: torch.Tensor) -> torch.Tensor:
    A, D, G, J, M = y.unbind(dim=-1)
    kf1, kf2, kf3, kf4, kr1, kr2, kr3, kr4 = k.unbind(dim=-1)

    dA = -kf1*A + kr1*D
    dD =  kf1*A - kr1*D - kf2*D + kr2*G
    dG =  kf2*D - kr2*G - kf3*G + kr3*J
    dJ =  kf3*G - kr3*J - kf4*J + kr4*M
    dM =  kf4*J - kr4*M
    return torch.stack([dA, dD, dG, dJ, dM], dim=-1)


# 7-state chain: [A, C, E, G, I, K, M]
def reduced7_rhs(y: torch.Tensor, k: torch.Tensor) -> torch.Tensor:
    A, C, E, G, I, K, M = y.unbind(dim=-1)
    kf1, kf2, kf3, kf4, kf5, kf6, kr1, kr2, kr3, kr4, kr5, kr6 = k.unbind(dim=-1)

    dA = -kf1*A + kr1*C
    dC =  kf1*A - kr1*C - kf2*C + kr2*E
    dE =  kf2*C - kr2*E - kf3*E + kr3*G
    dG =  kf3*E - kr3*G - kf4*G + kr4*I
    dI =  kf4*G - kr4*I - kf5*I + kr5*K
    dK =  kf5*I - kr5*K - kf6*K + kr6*M
    dM =  kf6*K - kr6*M
    return torch.stack([dA, dC, dE, dG, dI, dK, dM], dim=-1)


# 8-state chain: [A, B, D, F, H, J, L, M]
def reduced8_rhs(y: torch.Tensor, k: torch.Tensor) -> torch.Tensor:
    A, B, D, F, H, J, L, M = y.unbind(dim=-1)
    kf1, kf2, kf3, kf4, kf5, kf6, kf7, kr1, kr2, kr3, kr4, kr5, kr6, kr7 = k.unbind(dim=-1)

    dA = -kf1*A + kr1*B
    dB =  kf1*A - kr1*B - kf2*B + kr2*D
    dD =  kf2*B - kr2*D - kf3*D + kr3*F
    dF =  kf3*D - kr3*F - kf4*F + kr4*H
    dH =  kf4*F - kr4*H - kf5*H + kr5*J
    dJ =  kf5*H - kr5*J - kf6*J + kr6*L
    dL =  kf6*J - kr6*L - kf7*L + kr7*M
    dM =  kf7*L - kr7*M
    return torch.stack([dA, dB, dD, dF, dH, dJ, dL, dM], dim=-1)


# 9-state chain: [A, B, C, E, G, I, K, L, M]
def reduced9_rhs(y: torch.Tensor, k: torch.Tensor) -> torch.Tensor:
    A, B, C, E, G, I, K, L, M = y.unbind(dim=-1)
    kf1, kf2, kf3, kf4, kf5, kf6, kf7, kf8, kr1, kr2, kr3, kr4, kr5, kr6, kr7, kr8 = k.unbind(dim=-1)

    dA = -kf1*A + kr1*B
    dB =  kf1*A - kr1*B - kf2*B + kr2*C
    dC =  kf2*B - kr2*C - kf3*C + kr3*E
    dE =  kf3*C - kr3*E - kf4*E + kr4*G
    dG =  kf4*E - kr4*G - kf5*G + kr5*I
    dI =  kf5*G - kr5*I - kf6*I + kr6*K
    dK =  kf6*I - kr6*K - kf7*K + kr7*L
    dL =  kf7*K - kr7*L - kf8*L + kr8*M
    dM =  kf8*L - kr8*M
    return torch.stack([dA, dB, dC, dE, dG, dI, dK, dL, dM], dim=-1)

# Use a dictionary to store information about every mechanism.
MECH = {
    "full13": Mechanism(
        name="full13",
        n_state=13,
        n_param=19,
        state_names=list("ABCDEFGHIJKLM"),
        param_names=[
            "kf1","kf2","kf3","kf4","kf5","kf6","kf7","kf8","kf9","kf10","kf11","kf12",
            "kr1","kr3","kr5","kr7","kr9","kr11","kr12",
        ],
        rhs=full13_rhs,
    ),
    "reduced5": Mechanism(
        name="reduced5",
        n_state=5,
        n_param=8,
        state_names=["A","D","G","J","M"],
        param_names=["kf1","kf2","kf3","kf4","kr1","kr2","kr3","kr4"],
        rhs=reduced5_rhs,
    ),
    "reduced7": Mechanism(
        name="reduced7",
        n_state=7,
        n_param=12,
        state_names=["A","C","E","G","I","K","M"],
        param_names=["kf1","kf2","kf3","kf4","kf5","kf6","kr1","kr2","kr3","kr4","kr5","kr6"],
        rhs=reduced7_rhs,
    ),
    "reduced8": Mechanism(
        name="reduced8",
        n_state=8,
        n_param=14,
        state_names=["A","B","D","F","H","J","L","M"],
        param_names=["kf1","kf2","kf3","kf4","kf5","kf6","kf7","kr1","kr2","kr3","kr4","kr5","kr6","kr7"],
        rhs=reduced8_rhs,
    ),
    "reduced9": Mechanism(
        name="reduced9",
        n_state=9,
        n_param=16,
        state_names=["A","B","C","E","G","I","K","L","M"],
        param_names=["kf1","kf2","kf3","kf4","kf5","kf6","kf7","kf8","kr1","kr2","kr3","kr4","kr5","kr6","kr7","kr8"],
        rhs=reduced9_rhs,
    ),
}
